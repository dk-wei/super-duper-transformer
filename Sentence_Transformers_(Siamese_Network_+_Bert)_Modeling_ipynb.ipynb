{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Transformers (Siamese Network + Bert) Modeling.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fLtRIz45Ac-z"
      ],
      "authorship_tag": "ABX9TyM40NfyRLgNfwH9IsdLuyeR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16f950d51c804d57a9a4c3c95d9277d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a7d2064b2b24d5b85973ab6590ba527",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b917649becef4d7cae7857b411b9429e",
              "IPY_MODEL_0167110268594bdbb3863fb86f2b3e93"
            ]
          }
        },
        "5a7d2064b2b24d5b85973ab6590ba527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b917649becef4d7cae7857b411b9429e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bdd79ad4ff094688ae2706ccb7e9289c",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c11f61bd30c4497814d4f955c0312be"
          }
        },
        "0167110268594bdbb3863fb86f2b3e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e001c8113a94d698fd935e23c29449a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [01:06&lt;00:00, 66.15s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d72f742392b844b5aec7e09abf72990b"
          }
        },
        "bdd79ad4ff094688ae2706ccb7e9289c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c11f61bd30c4497814d4f955c0312be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e001c8113a94d698fd935e23c29449a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d72f742392b844b5aec7e09abf72990b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbcc8cd54a3841a59696885352c2021a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a35d426b5524baf9e17875366877320",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46a8fd048c2a4810ba055e61465f9651",
              "IPY_MODEL_714632c6006547189c6204dd67309b01"
            ]
          }
        },
        "5a35d426b5524baf9e17875366877320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46a8fd048c2a4810ba055e61465f9651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0bcb371917c64d78985e449ca649916e",
            "_dom_classes": [],
            "description": "Iteration: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_792c3ec6d87e4f3daf9a07495270663c"
          }
        },
        "714632c6006547189c6204dd67309b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8717974248fd46349b14c78ddae56452",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:04&lt;00:00,  2.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd6692a340c44048b0199261a8f10378"
          }
        },
        "0bcb371917c64d78985e449ca649916e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "792c3ec6d87e4f3daf9a07495270663c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8717974248fd46349b14c78ddae56452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd6692a340c44048b0199261a8f10378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0a68e8c71e3454aa673bfd6a3fcacaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9d5419adc1e140abafaa4f11d167e0c7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_36085aa8dd0c4158bb7a8654b762a394",
              "IPY_MODEL_03a98f6b315842c2b5c4e9b4df90f68e"
            ]
          }
        },
        "9d5419adc1e140abafaa4f11d167e0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36085aa8dd0c4158bb7a8654b762a394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_87cd8ec6656d4b7ba048b693b74fe315",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_928c284d1f204f268b0bad44f34c44f6"
          }
        },
        "03a98f6b315842c2b5c4e9b4df90f68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae464c82b81d4f0aa0b2385cb1e5f389",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [02:25&lt;00:00, 145.68s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ccd638804cb4ea0a8866f503f8e12b4"
          }
        },
        "87cd8ec6656d4b7ba048b693b74fe315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "928c284d1f204f268b0bad44f34c44f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae464c82b81d4f0aa0b2385cb1e5f389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ccd638804cb4ea0a8866f503f8e12b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "079a9bb30f364f04a80f74fd2f190732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_848b1b533fce492cb072adfc666fe8f2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d550e0f30ace4c58854a8b2188c8f6f4",
              "IPY_MODEL_a77756eeaced4ab68303ce1c4978b3d8"
            ]
          }
        },
        "848b1b533fce492cb072adfc666fe8f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d550e0f30ace4c58854a8b2188c8f6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_44f274f67ae54039a14a0c18b2b7ea5e",
            "_dom_classes": [],
            "description": "Iteration: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ceecf9bc82da4747818d2f015d17ae5f"
          }
        },
        "a77756eeaced4ab68303ce1c4978b3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb9b926f90bd401495887d9e91133295",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:07&lt;00:00,  3.91s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43ea459d6f1e46b799c0cd935f58af04"
          }
        },
        "44f274f67ae54039a14a0c18b2b7ea5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ceecf9bc82da4747818d2f015d17ae5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb9b926f90bd401495887d9e91133295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43ea459d6f1e46b799c0cd935f58af04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/super-duper-transformer/blob/main/Sentence_Transformers_(Siamese_Network_%2B_Bert)_Modeling_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-uUgZAjs-2_",
        "outputId": "c0a4335c-220e-4451-a295-f667e7fbebc0"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLtRIz45Ac-z"
      },
      "source": [
        "# Create Split Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0eTwgHPAXNb",
        "outputId": "175b99f5-b60b-446b-8c40-2e8b7a684dbb"
      },
      "source": [
        "\"\"\"\n",
        "The Quora Duplicate Questions dataset contains questions pairs from Quora (www.quora.com)\n",
        "along with a label whether the two questions are a duplicate, i.e., have an identical itention.\n",
        "Example of a duplicate pair:\n",
        "How do I enhance my English?  AND  How can I become good at English?\n",
        "Example of a non-duplicate pair:\n",
        "How are roads named?   AND    How are airport runways named?\n",
        "More details and the original Quora dataset can be found here:\n",
        "https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\n",
        "Dataset: http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\n",
        "You do not need to run this script. You can download all files from here:\n",
        "https://sbert.net/datasets/quora-duplicate-questions.zip\n",
        "This script does the following:\n",
        "1) After reading the quora_duplicate_questions.tsv, as provided by Quora, we add a transitive closure: If question (A, B) are duplicates and (B, C) are duplicates, than (A, C) must also be a duplicate. We add these missing links.\n",
        "2) Next, we split sentences into train, dev, and test with a ratio of about 85% / 5% / 10%. In contrast to must other Quora data splits, like the split provided by GLUE, we ensure that the three sets are overlap free, i.e., no sentences in dev / test will appear in the train dataset. In order to achieve three distinct datasets, we pick a sentence and then assign all duplicate sentences to this dataset to that repective set\n",
        "3) After distributing sentences to the three dataset split, we create files to facilitate 3 different tasks:\n",
        "    3.1) Classification - Given two sentences, are these a duplicate? This is identical to the orginial Quora task and the task in GLUE, but with the big difference that sentences in dev / test have not been seen in train.\n",
        "    3.2) Duplicate Question Mining - Given a large set of questions, identify all duplicates. The dev set consists of about 50k questions, the test set of about 100k sentences.\n",
        "    3.3) Information Retrieval - Given a question as query, find in a large corpus (~350k questions) the duplicates of the query question.\n",
        "The output consists of the following files:\n",
        "quora_duplicate_questions.tsv - Original file provided by Quora (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n",
        "classification/\n",
        "    train/dev/test_pairs.tsv - Distinct sets of question pairs with label for duplicate / non-duplicate. These splits can be used for sentence pair classification tasks\n",
        "duplicate-mining/ - Given a large set of questions, find all duplicates.\n",
        "    _corpus.tsv - Large set of sentences\n",
        "    _duplicates.tsv - All duplicate questions in the respective corpus.tsv\n",
        "information-retrieval/  - Given a large corpus of questions, find the duplicates for a given query\n",
        "    corpus.tsv - This file will be used for train/dev/test. It contains all questions in the corpus\n",
        "    dev/test-queries.tsv - Queries and the respective duplicate questions (QIDs) in the corpus\n",
        "\"\"\"\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import os\n",
        "from sentence_transformers import util\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "#Get raw file\n",
        "source_file = \"quora-IR-dataset/quora_duplicate_questions.tsv\"\n",
        "os.makedirs('quora-IR-dataset', exist_ok=True)\n",
        "os.makedirs('quora-IR-dataset/graph', exist_ok=True)\n",
        "os.makedirs('quora-IR-dataset/information-retrieval', exist_ok=True)\n",
        "os.makedirs('quora-IR-dataset/classification', exist_ok=True)\n",
        "os.makedirs('quora-IR-dataset/duplicate-mining', exist_ok=True)\n",
        "\n",
        "if not os.path.exists(source_file):\n",
        "    print(\"Download file to\", source_file)\n",
        "    util.http_get('http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv', source_file)\n",
        "\n",
        "#Read pairwise file\n",
        "sentences = {}\n",
        "duplicates = defaultdict(lambda: defaultdict(bool))\n",
        "rows = []\n",
        "with open(source_file, encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
        "    for row in reader:\n",
        "        id1 = row['qid1']\n",
        "        id2 = row['qid2']\n",
        "        question1 = row['question1'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        question2 = row['question2'].replace(\"\\r\", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "        is_duplicate = row['is_duplicate']\n",
        "\n",
        "        if question1 == \"\" or question2 == \"\":\n",
        "            continue\n",
        "\n",
        "        sentences[id1] = question1\n",
        "        sentences[id2] = question2\n",
        "\n",
        "        rows.append({'qid1': id1, 'qid2': id2, 'question1': question1, 'question2': question2, 'is_duplicate': is_duplicate})\n",
        "\n",
        "        if is_duplicate == '1':\n",
        "            duplicates[id1][id2] = True\n",
        "            duplicates[id2][id1] = True\n",
        "\n",
        "\n",
        "# Search for (near) exact duplicates\n",
        "# The original Quora duplicate questions dataset is an incomplete annotation,\n",
        "# i.e., there are several duplicate question pairs which are not marked as duplicates.\n",
        "# These missing annotation can make it difficult to compare approaches.\n",
        "# Here we use a simple approach that searches for near identical questions, that only differ in maybe a stopword\n",
        "# We mark these found question pairs also as duplicate to increase the annotation coverage\n",
        "stopwords = set(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'which', 'while', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'])\n",
        "\n",
        "num_new_duplicates = 0\n",
        "sentences_norm = {}\n",
        "\n",
        "for id, sent in sentences.items():\n",
        "    sent_norm = sent.lower()\n",
        "\n",
        "    #Replace some common paraphrases\n",
        "    sent_norm = sent_norm.replace(\"how do you\", \"how do i\").replace(\"how do we\", \"how do i\")\n",
        "    sent_norm = sent_norm.replace(\"how can we\", \"how can i\").replace(\"how can you\", \"how can i\").replace(\"how can i\", \"how do i\")\n",
        "    sent_norm = sent_norm.replace(\"really true\", \"true\")\n",
        "    sent_norm = sent_norm.replace(\"what are the importance\", \"what is the importance\")\n",
        "    sent_norm = sent_norm.replace(\"what was\", \"what is\")\n",
        "    sent_norm = sent_norm.replace(\"so many\", \"many\")\n",
        "    sent_norm = sent_norm.replace(\"would it take\", \"will it take\")\n",
        "\n",
        "    #Remove any punctuation characters\n",
        "    for c in [\",\", \"!\", \".\", \"?\", \"'\", '\"', \":\", \";\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\"]:\n",
        "        sent_norm = sent_norm.replace(c, \" \")\n",
        "\n",
        "    #Remove stop words\n",
        "    tokens = sent_norm.split()\n",
        "    tokens = [token for token in tokens if token not in stopwords]\n",
        "    sent_norm = \"\".join(tokens)\n",
        "\n",
        "\n",
        "    if sent_norm in sentences_norm:\n",
        "        if not duplicates[id][sentences_norm[sent_norm]]:\n",
        "            num_new_duplicates += 1\n",
        "\n",
        "        duplicates[id][sentences_norm[sent_norm]] = True\n",
        "        duplicates[sentences_norm[sent_norm]][id] = True\n",
        "    else:\n",
        "        sentences_norm[sent_norm] = id\n",
        "\n",
        "\n",
        "print(\"(Nearly) exact duplicates found:\", num_new_duplicates)\n",
        "\n",
        "\n",
        "#Add transitive closure (if a,b and b,c duplicates => a,c are duplicates)\n",
        "new_entries = True\n",
        "while new_entries:\n",
        "    print(\"Add transitive closure\")\n",
        "    new_entries = False\n",
        "    for a in sentences:\n",
        "        for b in list(duplicates[a]):\n",
        "            for c in list(duplicates[b]):\n",
        "                if a != c and not duplicates[a][c]:\n",
        "                    new_entries = True\n",
        "                    duplicates[a][c] = True\n",
        "                    duplicates[c][a] = True\n",
        "\n",
        "\n",
        "#Distribute rows to train/dev/test split\n",
        "#Ensure that sets contain distinct sentences\n",
        "is_assigned = set()\n",
        "random.shuffle(rows)\n",
        "\n",
        "train_ids = set()\n",
        "dev_ids = set()\n",
        "test_ids = set()\n",
        "\n",
        "counter = 0\n",
        "for row in rows:\n",
        "    if row['qid1'] in is_assigned and row['qid2'] in is_assigned:\n",
        "        continue\n",
        "    elif row['qid1'] in is_assigned or row['qid2'] in is_assigned:\n",
        "\n",
        "        if row['qid2'] in is_assigned: #Ensure that qid1 is assigned and qid2 not yet\n",
        "            row['qid1'], row['qid2'] = row['qid2'], row['qid1']\n",
        "\n",
        "        #Move qid2 to the same split as qid1\n",
        "        target_set = train_ids\n",
        "        if row['qid1'] in dev_ids:\n",
        "            target_set = dev_ids\n",
        "        elif row['qid1'] in test_ids:\n",
        "            target_set = test_ids\n",
        "\n",
        "    else:\n",
        "        #Distribution about 85%/5%/10%\n",
        "        target_set = train_ids\n",
        "        if counter%10 == 0:\n",
        "            target_set = dev_ids\n",
        "        elif counter%10 == 1 or counter%10 == 2:\n",
        "            target_set = test_ids\n",
        "        counter += 1\n",
        "\n",
        "    #Get the sentence with all duplicates and add it to the respective sets\n",
        "    target_set.add(row['qid1'])\n",
        "    is_assigned.add(row['qid1'])\n",
        "\n",
        "    target_set.add(row['qid2'])\n",
        "    is_assigned.add(row['qid2'])\n",
        "\n",
        "    for b in list(duplicates[row['qid1']])+list(duplicates[row['qid2']]):\n",
        "        target_set.add(b)\n",
        "        is_assigned.add(b)\n",
        "\n",
        "\n",
        "#Assert all sets are mutually exclusive\n",
        "assert len(train_ids.intersection(dev_ids)) == 0\n",
        "assert len(train_ids.intersection(test_ids)) == 0\n",
        "assert len(test_ids.intersection(dev_ids)) == 0\n",
        "\n",
        "\n",
        "print(\"\\nTrain sentences:\", len(train_ids))\n",
        "print(\"Dev sentences:\", len(dev_ids))\n",
        "print(\"Test sentences:\", len(test_ids))\n",
        "\n",
        "#Extract the ids for duplicate questions for train/dev/test\n",
        "def get_duplicate_set(ids_set):\n",
        "    dups_set = set()\n",
        "    for a in ids_set:\n",
        "        for b in duplicates[a]:\n",
        "            ids = sorted([a,b])\n",
        "            dups_set.add(tuple(ids))\n",
        "    return dups_set\n",
        "\n",
        "train_duplicates = get_duplicate_set(train_ids)\n",
        "dev_duplicates = get_duplicate_set(dev_ids)\n",
        "test_duplicates = get_duplicate_set(test_ids)\n",
        "\n",
        "\n",
        "print(\"\\nTrain duplicates\", len(train_duplicates))\n",
        "print(\"Dev duplicates\", len(dev_duplicates))\n",
        "print(\"Test duplicates\", len(test_duplicates))\n",
        "\n",
        "############### Write general files about the duplate questions graph ############\n",
        "with open('quora-IR-dataset/graph/sentences.tsv', 'w', encoding='utf8') as fOut:\n",
        "    fOut.write(\"qid\\tquestion\\n\")\n",
        "    for id, question in sentences.items():\n",
        "        fOut.write(\"{}\\t{}\\n\".format(id, question))\n",
        "\n",
        "duplicates_list = set()\n",
        "for a in duplicates:\n",
        "    for b in duplicates[a]:\n",
        "        duplicates_list.add(tuple(sorted([int(a), int(b)])))\n",
        "\n",
        "\n",
        "duplicates_list = list(duplicates_list)\n",
        "duplicates_list = sorted(duplicates_list, key=lambda x: x[0]*1000000+x[1])\n",
        "\n",
        "\n",
        "print(\"\\nWrite duplicate graph in pairwise format\")\n",
        "with open('quora-IR-dataset/graph/duplicates-graph-pairwise.tsv', 'w', encoding='utf8') as fOut:\n",
        "    fOut.write(\"qid1\\tqid2\\n\")\n",
        "    for a, b in duplicates_list:\n",
        "        fOut.write(\"{}\\t{}\\n\".format(a, b))\n",
        "\n",
        "\n",
        "print(\"Write duplicate graph in list format\")\n",
        "with open('quora-IR-dataset/graph/duplicates-graph-list.tsv', 'w', encoding='utf8') as fOut:\n",
        "    fOut.write(\"qid1\\tqid2\\n\")\n",
        "    for a in sorted(duplicates.keys(), key=lambda x: int(x)):\n",
        "        if len(duplicates[a]) > 0:\n",
        "            fOut.write(\"{}\\t{}\\n\".format(a, \",\".join(sorted(duplicates[a]))))\n",
        "\n",
        "print(\"Write duplicate graph in connected subgraph format\")\n",
        "with open('quora-IR-dataset/graph/duplicates-graph-connected-nodes.tsv', 'w', encoding='utf8') as fOut:\n",
        "    written_qids = set()\n",
        "    fOut.write(\"qids\\n\")\n",
        "    for a in sorted(duplicates.keys(), key=lambda x: int(x)):\n",
        "        if a not in written_qids:\n",
        "            ids = set()\n",
        "            ids.add(a)\n",
        "\n",
        "            for b in duplicates[a]:\n",
        "                ids.add(b)\n",
        "\n",
        "            fOut.write(\"{}\\n\".format(\",\".join(sorted(ids, key=lambda x: int(x)))))\n",
        "            for id in ids:\n",
        "                written_qids.add(id)\n",
        "\n",
        "def write_qids(name, ids_list):\n",
        "    with open('quora-IR-dataset/graph/'+name+'-questions.tsv', 'w', encoding='utf8') as fOut:\n",
        "        fOut.write(\"qid\\n\")\n",
        "        fOut.write(\"\\n\".join(sorted(ids_list, key=lambda x: int(x))))\n",
        "\n",
        "write_qids('train', train_ids)\n",
        "write_qids('dev', dev_ids)\n",
        "write_qids('test', test_ids)\n",
        "\n",
        "\n",
        "####### Output for duplicate mining #######\n",
        "def write_mining_files(name, ids, dups):\n",
        "    with open('quora-IR-dataset/duplicate-mining/'+name+'_corpus.tsv', 'w', encoding='utf8') as fOut:\n",
        "        fOut.write(\"qid\\tquestion\\n\")\n",
        "        for id in ids:\n",
        "            fOut.write(\"{}\\t{}\\n\".format(id, sentences[id]))\n",
        "\n",
        "    with open('quora-IR-dataset/duplicate-mining/'+name+'_duplicates.tsv', 'w', encoding='utf8') as fOut:\n",
        "        fOut.write(\"qid1\\tqid2\\n\")\n",
        "        for a, b in dups:\n",
        "            fOut.write(\"{}\\t{}\\n\".format(a, b))\n",
        "\n",
        "\n",
        "write_mining_files('train', train_ids, train_duplicates)\n",
        "write_mining_files('dev', dev_ids, dev_duplicates)\n",
        "write_mining_files('test', test_ids, test_duplicates)\n",
        "\n",
        "\n",
        "###### Classification dataset #####\n",
        "with open('quora-IR-dataset/classification/train_pairs.tsv', 'w', encoding='utf8') as fOutTrain, open('quora-IR-dataset/classification/dev_pairs.tsv', 'w', encoding='utf8') as fOutDev, open('quora-IR-dataset/classification/test_pairs.tsv', 'w', encoding='utf8') as fOutTest:\n",
        "    fOutTrain.write(\"\\t\".join(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate'])+\"\\n\")\n",
        "    fOutDev.write(\"\\t\".join(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate']) + \"\\n\")\n",
        "    fOutTest.write(\"\\t\".join(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate']) + \"\\n\")\n",
        "\n",
        "    for row in rows:\n",
        "        id1 = row['qid1']\n",
        "        id2 = row['qid2']\n",
        "\n",
        "        target = None\n",
        "        if id1 in train_ids and id2 in train_ids:\n",
        "            target = fOutTrain\n",
        "        elif id1 in dev_ids and id2 in dev_ids:\n",
        "            target = fOutDev\n",
        "        elif id1 in test_ids and id2 in test_ids:\n",
        "            target = fOutTest\n",
        "\n",
        "        if target is not None:\n",
        "            target.write(\"\\t\".join([row['qid1'], row['qid2'], sentences[id1], sentences[id2], row['is_duplicate']]))\n",
        "            target.write(\"\\n\")\n",
        "\n",
        "\n",
        "####### Write files for Information Retrieval #####\n",
        "num_dev_queries = 5000\n",
        "num_test_queries = 10000\n",
        "\n",
        "corpus_ids = train_ids.copy()\n",
        "dev_queries = set()\n",
        "test_queries = set()\n",
        "\n",
        "#Create dev queries\n",
        "rnd_dev_ids = sorted(list(dev_ids))\n",
        "random.shuffle(rnd_dev_ids)\n",
        "\n",
        "for a in rnd_dev_ids:\n",
        "    if a not in corpus_ids:\n",
        "        if len(dev_queries) < num_dev_queries and len(duplicates[a]) > 0:\n",
        "            dev_queries.add(a)\n",
        "        else:\n",
        "            corpus_ids.add(a)\n",
        "\n",
        "        for b in duplicates[a]:\n",
        "            if b not in dev_queries:\n",
        "                corpus_ids.add(b)\n",
        "\n",
        "#Create test queries\n",
        "rnd_test_ids = sorted(list(test_ids))\n",
        "random.shuffle(rnd_test_ids)\n",
        "\n",
        "for a in rnd_test_ids:\n",
        "    if a not in corpus_ids:\n",
        "        if len(test_queries) < num_test_queries and len(duplicates[a]) > 0:\n",
        "            test_queries.add(a)\n",
        "        else:\n",
        "            corpus_ids.add(a)\n",
        "\n",
        "        for b in duplicates[a]:\n",
        "            if b not in test_queries:\n",
        "                corpus_ids.add(b)\n",
        "\n",
        "#Write output for information-retrieval\n",
        "print(\"\\nInformation Retrival Setup\")\n",
        "print(\"Corpus size:\", len(corpus_ids))\n",
        "print(\"Dev queries:\", len(dev_queries))\n",
        "print(\"Test queries:\", len(test_queries))\n",
        "\n",
        "with open('quora-IR-dataset/information-retrieval/corpus.tsv', 'w', encoding='utf8') as fOut:\n",
        "    fOut.write(\"qid\\tquestion\\n\")\n",
        "    for id in sorted(corpus_ids, key=lambda id: int(id)):\n",
        "        fOut.write(\"{}\\t{}\\n\".format(id, sentences[id]))\n",
        "\n",
        "with open('quora-IR-dataset/information-retrieval/dev-queries.tsv', 'w', encoding='utf8') as fOut:\n",
        "    fOut.write(\"qid\\tquestion\\tduplicate_qids\\n\")\n",
        "    for id in sorted(dev_queries, key=lambda id: int(id)):\n",
        "        fOut.write(\"{}\\t{}\\t{}\\n\".format(id, sentences[id], \",\".join(duplicates[id])))\n",
        "\n",
        "with open('quora-IR-dataset/information-retrieval/test-queries.tsv', 'w', encoding='utf8') as fOut:\n",
        "    fOut.write(\"qid\\tquestion\\tduplicate_qids\\n\")\n",
        "    for id in sorted(test_queries, key=lambda id: int(id)):\n",
        "        fOut.write(\"{}\\t{}\\t{}\\n\".format(id, sentences[id], \",\".join(duplicates[id])))\n",
        "\n",
        "\n",
        "print(\"--DONE--\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Nearly) exact duplicates found: 6328\n",
            "Add transitive closure\n",
            "Add transitive closure\n",
            "Add transitive closure\n",
            "\n",
            "Train sentences: 376493\n",
            "Dev sentences: 53485\n",
            "Test sentences: 107953\n",
            "\n",
            "Train duplicates 217838\n",
            "Dev duplicates 20017\n",
            "Test duplicates 65350\n",
            "\n",
            "Write duplicate graph in pairwise format\n",
            "Write duplicate graph in list format\n",
            "Write duplicate graph in connected subgraph format\n",
            "\n",
            "Information Retrival Setup\n",
            "Corpus size: 522931\n",
            "Dev queries: 5000\n",
            "Test queries: 10000\n",
            "--DONE--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71iv5LmCvQyH"
      },
      "source": [
        "# Quora Duplicate Questions Training (MultipleNegativesRankingLoss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WZOcOUexIJP",
        "outputId": "77f75b57-7b46-4c3e-ecc0-597b96c363f1"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses, util\n",
        "from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n",
        "from sentence_transformers.readers import InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "logger = logging.getLogger(__name__)\n",
        "#### /print debug information to stdout\n",
        "\n",
        "\n",
        "#As base model, we use DistilBERT-base that was pre-trained on NLI and STSb data\n",
        "model = SentenceTransformer('stsb-distilbert-base')\n",
        "#model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')\n",
        "\n",
        "#Training for multiple epochs can be beneficial, as in each epoch a mini-batch is sampled differently\n",
        "#hence, we get different negatives for each positive\n",
        "num_epochs = 1\n",
        "\n",
        "#Increasing the batch size improves the performance for MultipleNegativesRankingLoss. Choose it as large as possible\n",
        "#I achieved the good results with a batch size of 300-350 (requires about 30 GB of GPU memory)\n",
        "train_batch_size = 64\n",
        "\n",
        "dataset_path = 'quora-IR-dataset'\n",
        "model_save_path = 'output/training_MultipleNegativesRankingLoss-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "#eval_save_path = 'output/evaluation_MultipleNegativesRankingLoss-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Check if the dataset exists. If not, download and extract\n",
        "if not os.path.exists(dataset_path):\n",
        "    logger.info(\"Dataset not found. Download\")\n",
        "    zip_save_path = 'quora-IR-dataset.zip'\n",
        "    util.http_get(url='https://sbert.net/datasets/quora-IR-dataset.zip', path=zip_save_path)\n",
        "    with ZipFile(zip_save_path, 'r') as zip:\n",
        "        zip.extractall(dataset_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-19 02:09:21 - Load pretrained SentenceTransformer: stsb-distilbert-base\n",
            "2021-03-19 02:09:21 - Did not find folder stsb-distilbert-base\n",
            "2021-03-19 02:09:21 - Search model on server: http://sbert.net/models/stsb-distilbert-base.zip\n",
            "2021-03-19 02:09:21 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_stsb-distilbert-base\n",
            "2021-03-19 02:09:22 - Use pytorch device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnY7ejP2Izzl"
      },
      "source": [
        "######### Read train data  ##########\n",
        "train_samples = []\n",
        "max_train_samples = 100\n",
        "\n",
        "with open(os.path.join(dataset_path, \"classification/train_pairs.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "      # Counter += 1\n",
        "      if row['is_duplicate'] == '1':\n",
        "          train_samples.append(InputExample(texts=[row['question1'], row['question2']], label=1))\n",
        "          train_samples.append(InputExample(texts=[row['question2'], row['question1']], label=1)) #if A is a duplicate of B, then B is a duplicate of A\n",
        "\n",
        "          if len(train_samples) >= max_train_samples:\n",
        "            break\n",
        "\n",
        "# After reading the train_samples, we create a DataLoader\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q85NJEgJIzwm"
      },
      "source": [
        "################### Development  Evaluators ##################\n",
        "# We add 3 evaluators, that evaluate the model on Duplicate Questions pair classification,\n",
        "# Duplicate Questions Mining, and Duplicate Questions Information Retrieval\n",
        "evaluators = []"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92cjI5cqLyGY"
      },
      "source": [
        "###### Classification ######\n",
        "# Given (quesiton1, question2), is this a duplicate or not?\n",
        "# The evaluator will compute the embeddings for both questions and then compute\n",
        "# a cosine similarity. If the similarity is above a threshold, we have a duplicate.\n",
        "dev_sentences1 = []\n",
        "dev_sentences2 = []\n",
        "dev_labels = []\n",
        "max_dev_pair_samples = 1000\n",
        "\n",
        "with open(os.path.join(dataset_path, \"classification/dev_pairs.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        dev_sentences1.append(row['question1'])\n",
        "        dev_sentences2.append(row['question2'])\n",
        "        dev_labels.append(int(row['is_duplicate']))\n",
        "\n",
        "        if len(dev_sentences1) >= max_dev_pair_samples:\n",
        "            break\n",
        "\n",
        "\n",
        "binary_acc_evaluator = evaluation.BinaryClassificationEvaluator(dev_sentences1, dev_sentences2, dev_labels)\n",
        "evaluators.append(binary_acc_evaluator)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eiQFmRbup2v"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcnBsMX3Izt4"
      },
      "source": [
        "###### Duplicate Questions Mining ######\n",
        "# Given a large corpus of questions, identify all duplicates in that corpus.\n",
        "# For faster processing, we limit the development corpus to only 10,000 sentences.\n",
        "max_dev_samples = 100000\n",
        "dev_sentences = {}\n",
        "dev_duplicates = []\n",
        "\n",
        "with open(os.path.join(dataset_path, \"duplicate-mining/dev_corpus.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        dev_sentences[row['qid']] = row['question']\n",
        "\n",
        "        if len(dev_sentences) >= max_dev_samples:\n",
        "            break\n",
        "\n",
        "with open(os.path.join(dataset_path, \"duplicate-mining/dev_duplicates.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        if row['qid1'] in dev_sentences and row['qid2'] in dev_sentences:\n",
        "            dev_duplicates.append([row['qid1'], row['qid2']])\n",
        "\n",
        "# The ParaphraseMiningEvaluator computes the cosine similarity between all sentences and\n",
        "# extracts a list with the pairs that have the highest similarity. Given the duplicate\n",
        "# information in dev_duplicates, it then computes and F1 score how well our duplicate mining worked\n",
        "paraphrase_mining_evaluator = evaluation.ParaphraseMiningEvaluator(dev_sentences, dev_duplicates, name='dev')\n",
        "evaluators.append(paraphrase_mining_evaluator)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH0ABmgwtIEV"
      },
      "source": [
        "###### Duplicate Questions Information Retrieval ######\n",
        "# Given a new question and a large corpus of thousands questions, find the most relevant (i.e. duplicate) question\n",
        "# in that corpus.\n",
        "\n",
        "# For faster processing, we limit the development corpus to only 10,000 sentences.\n",
        "max_corpus_size = 100000\n",
        "\n",
        "ir_queries = {}             #Our queries (qid => question)\n",
        "ir_needed_qids = set()      #QIDs we need in the corpus\n",
        "ir_corpus = {}              #Our corpus (qid => question)\n",
        "ir_relevant_docs = {}       #Mapping of relevant documents for a given query (qid => set([relevant_question_ids])\n",
        "\n",
        "with open(os.path.join(dataset_path, 'information-retrieval/dev-queries.tsv'), encoding='utf8') as fIn:\n",
        "    next(fIn) #Skip header\n",
        "    for line in fIn:\n",
        "        qid, query, duplicate_ids = line.strip().split('\\t')\n",
        "        duplicate_ids = duplicate_ids.split(',')\n",
        "        ir_queries[qid] = query\n",
        "        ir_relevant_docs[qid] = set(duplicate_ids)\n",
        "\n",
        "        for qid in duplicate_ids:\n",
        "            ir_needed_qids.add(qid)\n",
        "\n",
        "# First get all needed relevant documents (i.e., we must ensure, that the relevant questions are actually in the corpus\n",
        "distraction_questions = {}\n",
        "with open(os.path.join(dataset_path, 'information-retrieval/corpus.tsv'), encoding='utf8') as fIn:\n",
        "    next(fIn) #Skip header\n",
        "    for line in fIn:\n",
        "        qid, question = line.strip().split('\\t')\n",
        "\n",
        "        if qid in ir_needed_qids:\n",
        "            ir_corpus[qid] = question\n",
        "        else:\n",
        "            distraction_questions[qid] = question\n",
        "\n",
        "# Now, also add some irrelevant questions to fill our corpus\n",
        "other_qid_list = list(distraction_questions.keys())\n",
        "random.shuffle(other_qid_list)\n",
        "\n",
        "for qid in other_qid_list[0:max(0, max_corpus_size-len(ir_corpus))]:\n",
        "    ir_corpus[qid] = distraction_questions[qid]\n",
        "\n",
        "#Given queries, a corpus and a mapping with relevant documents, the InformationRetrievalEvaluator computes different IR\n",
        "# metrices. For our use case MRR@k and Accuracy@k are relevant.\n",
        "ir_evaluator = evaluation.InformationRetrievalEvaluator(ir_queries, ir_corpus, ir_relevant_docs)\n",
        "\n",
        "evaluators.append(ir_evaluator)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wopmOdd2RNkb",
        "outputId": "d92c8fed-4bf1-40bd-cac6-8f47d1503b27"
      },
      "source": [
        "evaluators"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<sentence_transformers.evaluation.BinaryClassificationEvaluator.BinaryClassificationEvaluator at 0x7f23c9924fd0>,\n",
              " <sentence_transformers.evaluation.ParaphraseMiningEvaluator.ParaphraseMiningEvaluator at 0x7f242b9d5910>,\n",
              " <sentence_transformers.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator at 0x7f242db8c510>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438,
          "referenced_widgets": [
            "16f950d51c804d57a9a4c3c95d9277d6",
            "5a7d2064b2b24d5b85973ab6590ba527",
            "b917649becef4d7cae7857b411b9429e",
            "0167110268594bdbb3863fb86f2b3e93",
            "bdd79ad4ff094688ae2706ccb7e9289c",
            "0c11f61bd30c4497814d4f955c0312be",
            "4e001c8113a94d698fd935e23c29449a",
            "d72f742392b844b5aec7e09abf72990b",
            "bbcc8cd54a3841a59696885352c2021a",
            "5a35d426b5524baf9e17875366877320",
            "46a8fd048c2a4810ba055e61465f9651",
            "714632c6006547189c6204dd67309b01",
            "0bcb371917c64d78985e449ca649916e",
            "792c3ec6d87e4f3daf9a07495270663c",
            "8717974248fd46349b14c78ddae56452",
            "bd6692a340c44048b0199261a8f10378"
          ]
        },
        "id": "caJp75Lgr-Zr",
        "outputId": "9f0008e2-86d7-44a8-884f-35251b71c51b"
      },
      "source": [
        "# Create a SequentialEvaluator. This SequentialEvaluator runs all three evaluators in a sequential order.\n",
        "# We optimize the model with respect to the score from the last evaluator (scores[-1])\n",
        "seq_evaluator = evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1])\n",
        "\n",
        "logger.info(\"Evaluate model without training\")\n",
        "#seq_evaluator(model, epoch=1, steps=0, output_path=model_save_path)\n",
        "model.evaluate(evaluators[1])\n",
        "\n",
        "logger.info(\"Evaluate model with training\")\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          #evaluator=seq_evaluator,\n",
        "          evaluator=evaluators[1],\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=1000,\n",
        "          output_path=model_save_path\n",
        "          )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-19 02:09:24 - Evaluate model without training\n",
            "2021-03-19 02:09:24 - Paraphrase Mining Evaluation on dev dataset:\n",
            "2021-03-19 02:10:21 - Number of candidate pairs: 265140\n",
            "2021-03-19 02:10:22 - Average Precision: 42.00\n",
            "2021-03-19 02:10:22 - Optimal threshold: 0.8632\n",
            "2021-03-19 02:10:22 - Precision: 48.78\n",
            "2021-03-19 02:10:22 - Recall: 43.95\n",
            "2021-03-19 02:10:22 - F1: 46.24\n",
            "\n",
            "2021-03-19 02:10:22 - Evaluate model with training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16f950d51c804d57a9a4c3c95d9277d6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='iâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbcc8cd54a3841a59696885352c2021a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2.0, style=ProgressStyle(description_widtâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2021-03-19 02:10:22 - Paraphrase Mining Evaluation on dev dataset after epoch 0:\n",
            "2021-03-19 02:11:18 - Number of candidate pairs: 265139\n",
            "2021-03-19 02:11:18 - Average Precision: 41.99\n",
            "2021-03-19 02:11:18 - Optimal threshold: 0.8632\n",
            "2021-03-19 02:11:18 - Precision: 48.78\n",
            "2021-03-19 02:11:18 - Recall: 43.95\n",
            "2021-03-19 02:11:18 - F1: 46.24\n",
            "\n",
            "2021-03-19 02:11:19 - Save model to output/training_MultipleNegativesRankingLoss-2021-03-19_02-09-22\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPG4c3-n26kH"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAsB-M4b26hO"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6WZc7EM26fq"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAZzhrYr26cQ"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB339F-V26ZE"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vfB3c3H_yNz"
      },
      "source": [
        "# Quora Duplicate Questions Training (OnlineContrastiveLoss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XOwXe4EtH5T",
        "outputId": "1cde22e6-aa40-425f-cec8-40c763002f52"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses, util\n",
        "from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n",
        "from sentence_transformers.readers import InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "logger = logging.getLogger(__name__)\n",
        "#### /print debug information to stdout\n",
        "\n",
        "\n",
        "#As base model, we use DistilBERT-base that was pre-trained on NLI and STSb data\n",
        "model = SentenceTransformer('stsb-distilbert-base')\n",
        "num_epochs = 1\n",
        "train_batch_size = 64\n",
        "\n",
        "#As distance metric, we use cosine distance (cosine_distance = 1-cosine_similarity)\n",
        "distance_metric = losses.SiameseDistanceMetric.COSINE_DISTANCE\n",
        "\n",
        "#Negative pairs should have a distance of at least 0.5\n",
        "margin = 0.5\n",
        "\n",
        "dataset_path = 'quora-IR-dataset'\n",
        "model_save_path = 'output/training_OnlineConstrativeLoss-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "eval_save_path = 'output/evaluation_OnlineConstrativeLoss-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Check if the dataset exists. If not, download and extract\n",
        "if not os.path.exists(dataset_path):\n",
        "    logger.info(\"Dataset not found. Download\")\n",
        "    zip_save_path = 'quora-IR-dataset.zip'\n",
        "    util.http_get(url='https://sbert.net/datasets/quora-IR-dataset.zip', path=zip_save_path)\n",
        "    with ZipFile(zip_save_path, 'r') as zip:\n",
        "        zip.extractall(dataset_path)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-19 02:11:19 - Load pretrained SentenceTransformer: stsb-distilbert-base\n",
            "2021-03-19 02:11:19 - Did not find folder stsb-distilbert-base\n",
            "2021-03-19 02:11:19 - Search model on server: http://sbert.net/models/stsb-distilbert-base.zip\n",
            "2021-03-19 02:11:19 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_stsb-distilbert-base\n",
            "2021-03-19 02:11:20 - Use pytorch device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibqrxm1Q3wiK"
      },
      "source": [
        "######### Read train data  ##########\n",
        "# Read train data\n",
        "max_train_samples = 100\n",
        "train_samples = []\n",
        "\n",
        "\n",
        "with open(os.path.join(dataset_path, \"classification/train_pairs.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        sample = InputExample(texts=[row['question1'], row['question2']], label=int(row['is_duplicate']))\n",
        "        train_samples.append(sample)\n",
        "\n",
        "        if len(train_samples) >= max_train_samples:\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.OnlineContrastiveLoss(model=model, distance_metric=distance_metric, margin=margin)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB5x7NVLAEbA"
      },
      "source": [
        "################### Development  Evaluators ##################\n",
        "# We add 3 evaluators, that evaluate the model on Duplicate Questions pair classification,\n",
        "# Duplicate Questions Mining, and Duplicate Questions Information Retrieval\n",
        "evaluators = []\n",
        "\n",
        "###### Classification ######\n",
        "# Given (quesiton1, question2), is this a duplicate or not?\n",
        "# The evaluator will compute the embeddings for both questions and then compute\n",
        "# a cosine similarity. If the similarity is above a threshold, we have a duplicate.\n",
        "dev_sentences1 = []\n",
        "dev_sentences2 = []\n",
        "dev_labels = []\n",
        "with open(os.path.join(dataset_path, \"classification/dev_pairs.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        dev_sentences1.append(row['question1'])\n",
        "        dev_sentences2.append(row['question2'])\n",
        "        dev_labels.append(int(row['is_duplicate']))\n",
        "\n",
        "\n",
        "binary_acc_evaluator = evaluation.BinaryClassificationEvaluator(dev_sentences1, dev_sentences2, dev_labels)\n",
        "evaluators.append(binary_acc_evaluator)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwiOZ6neAH59"
      },
      "source": [
        "###### Duplicate Questions Mining ######\n",
        "# Given a large corpus of questions, identify all duplicates in that corpus.\n",
        "\n",
        "# For faster processing, we limit the development corpus to only 10,000 sentences.\n",
        "max_dev_samples = 10000\n",
        "dev_sentences = {}\n",
        "dev_duplicates = []\n",
        "with open(os.path.join(dataset_path, \"duplicate-mining/dev_corpus.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        dev_sentences[row['qid']] = row['question']\n",
        "\n",
        "        if len(dev_sentences) >= max_dev_samples:\n",
        "            break\n",
        "\n",
        "with open(os.path.join(dataset_path, \"duplicate-mining/dev_duplicates.tsv\"), encoding='utf8') as fIn:\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "    for row in reader:\n",
        "        if row['qid1'] in dev_sentences and row['qid2'] in dev_sentences:\n",
        "            dev_duplicates.append([row['qid1'], row['qid2']])\n",
        "\n",
        "\n",
        "# The ParaphraseMiningEvaluator computes the cosine similarity between all sentences and\n",
        "# extracts a list with the pairs that have the highest similarity. Given the duplicate\n",
        "# information in dev_duplicates, it then computes and F1 score how well our duplicate mining worked\n",
        "paraphrase_mining_evaluator = evaluation.ParaphraseMiningEvaluator(dev_sentences, dev_duplicates, name='dev')\n",
        "evaluators.append(paraphrase_mining_evaluator)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NcGD-F-ANVQ"
      },
      "source": [
        "###### Duplicate Questions Information Retrieval ######\n",
        "# Given a question and a large corpus of thousands questions, find the most relevant (i.e. duplicate) question\n",
        "# in that corpus.\n",
        "\n",
        "# For faster processing, we limit the development corpus to only 10,000 sentences.\n",
        "max_corpus_size = 100000\n",
        "\n",
        "ir_queries = {}             #Our queries (qid => question)\n",
        "ir_needed_qids = set()      #QIDs we need in the corpus\n",
        "ir_corpus = {}              #Our corpus (qid => question)\n",
        "ir_relevant_docs = {}       #Mapping of relevant documents for a given query (qid => set([relevant_question_ids])\n",
        "\n",
        "with open(os.path.join(dataset_path, 'information-retrieval/dev-queries.tsv'), encoding='utf8') as fIn:\n",
        "    next(fIn) #Skip header\n",
        "    for line in fIn:\n",
        "        qid, query, duplicate_ids = line.strip().split('\\t')\n",
        "        duplicate_ids = duplicate_ids.split(',')\n",
        "        ir_queries[qid] = query\n",
        "        ir_relevant_docs[qid] = set(duplicate_ids)\n",
        "\n",
        "        for qid in duplicate_ids:\n",
        "            ir_needed_qids.add(qid)\n",
        "\n",
        "# First get all needed relevant documents (i.e., we must ensure, that the relevant questions are actually in the corpus\n",
        "distraction_questions = {}\n",
        "with open(os.path.join(dataset_path, 'information-retrieval/corpus.tsv'), encoding='utf8') as fIn:\n",
        "    next(fIn) #Skip header\n",
        "    for line in fIn:\n",
        "        qid, question = line.strip().split('\\t')\n",
        "\n",
        "        if qid in ir_needed_qids:\n",
        "            ir_corpus[qid] = question\n",
        "        else:\n",
        "            distraction_questions[qid] = question\n",
        "\n",
        "# Now, also add some irrelevant questions to fill our corpus\n",
        "other_qid_list = list(distraction_questions.keys())\n",
        "random.shuffle(other_qid_list)\n",
        "\n",
        "for qid in other_qid_list[0:max(0, max_corpus_size-len(ir_corpus))]:\n",
        "    ir_corpus[qid] = distraction_questions[qid]\n",
        "\n",
        "#Given queries, a corpus and a mapping with relevant documents, the InformationRetrievalEvaluator computes different IR\n",
        "# metrices. For our use case MRR@k and Accuracy@k are relevant.\n",
        "ir_evaluator = evaluation.InformationRetrievalEvaluator(ir_queries, ir_corpus, ir_relevant_docs)\n",
        "\n",
        "evaluators.append(ir_evaluator)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e0a68e8c71e3454aa673bfd6a3fcacaf",
            "9d5419adc1e140abafaa4f11d167e0c7",
            "36085aa8dd0c4158bb7a8654b762a394",
            "03a98f6b315842c2b5c4e9b4df90f68e",
            "87cd8ec6656d4b7ba048b693b74fe315",
            "928c284d1f204f268b0bad44f34c44f6",
            "ae464c82b81d4f0aa0b2385cb1e5f389",
            "3ccd638804cb4ea0a8866f503f8e12b4",
            "079a9bb30f364f04a80f74fd2f190732",
            "848b1b533fce492cb072adfc666fe8f2",
            "d550e0f30ace4c58854a8b2188c8f6f4",
            "a77756eeaced4ab68303ce1c4978b3d8",
            "44f274f67ae54039a14a0c18b2b7ea5e",
            "ceecf9bc82da4747818d2f015d17ae5f",
            "fb9b926f90bd401495887d9e91133295",
            "43ea459d6f1e46b799c0cd935f58af04"
          ]
        },
        "id": "TB5cTAioAQYW",
        "outputId": "bda0db6d-ede6-4fa9-df98-7345983cd2f7"
      },
      "source": [
        "# Create a SequentialEvaluator. This SequentialEvaluator runs all three evaluators in a sequential order.\n",
        "# We optimize the model with respect to the score from the last evaluator (scores[-1])\n",
        "seq_evaluator = evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1])\n",
        "\n",
        "\n",
        "logger.info(\"Evaluate model without training\")\n",
        "seq_evaluator(model, epoch=0, steps=0, output_path=model_save_path)\n",
        "\n",
        "logger.info(\"Evaluate model with training\")\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=seq_evaluator,\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=1000,\n",
        "          save_best_model=True,  # If true, the best model (according to evaluator) is stored at output_path\n",
        "          output_path=model_save_path\n",
        "          )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-19 02:11:22 - Evaluate model without training\n",
            "2021-03-19 02:11:22 - Binary Accuracy Evaluation of the model on  dataset in epoch 0 after 0 steps:\n",
            "2021-03-19 02:12:15 - Accuracy with Cosine-Similarity:           76.61\t(Threshold: 0.8349)\n",
            "2021-03-19 02:12:15 - F1 with Cosine-Similarity:                 69.29\t(Threshold: 0.7391)\n",
            "2021-03-19 02:12:15 - Precision with Cosine-Similarity:          60.18\n",
            "2021-03-19 02:12:15 - Recall with Cosine-Similarity:             81.64\n",
            "2021-03-19 02:12:15 - Average Precision with Cosine-Similarity:  71.80\n",
            "\n",
            "2021-03-19 02:12:16 - Accuracy with Manhatten-Distance:           76.70\t(Threshold: 193.7356)\n",
            "2021-03-19 02:12:16 - F1 with Manhatten-Distance:                 69.44\t(Threshold: 240.4903)\n",
            "2021-03-19 02:12:16 - Precision with Manhatten-Distance:          59.75\n",
            "2021-03-19 02:12:16 - Recall with Manhatten-Distance:             82.88\n",
            "2021-03-19 02:12:16 - Average Precision with Manhatten-Distance:  71.87\n",
            "\n",
            "2021-03-19 02:12:16 - Accuracy with Euclidean-Distance:           76.75\t(Threshold: 8.8834)\n",
            "2021-03-19 02:12:16 - F1 with Euclidean-Distance:                 69.45\t(Threshold: 10.8947)\n",
            "2021-03-19 02:12:16 - Precision with Euclidean-Distance:          59.90\n",
            "2021-03-19 02:12:16 - Recall with Euclidean-Distance:             82.62\n",
            "2021-03-19 02:12:16 - Average Precision with Euclidean-Distance:  71.90\n",
            "\n",
            "2021-03-19 02:12:16 - Paraphrase Mining Evaluation on dev dataset in epoch 0 after 0 steps:\n",
            "2021-03-19 02:12:31 - Number of candidate pairs: 294936\n",
            "2021-03-19 02:12:31 - Average Precision: 40.91\n",
            "2021-03-19 02:12:31 - Optimal threshold: 0.8637\n",
            "2021-03-19 02:12:31 - Precision: 46.41\n",
            "2021-03-19 02:12:31 - Recall: 45.03\n",
            "2021-03-19 02:12:31 - F1: 45.71\n",
            "\n",
            "2021-03-19 02:12:32 - Information Retrieval Evaluation on  dataset in epoch 0 after 0 steps:\n",
            "2021-03-19 02:13:47 - Queries: 5000\n",
            "2021-03-19 02:13:47 - Corpus: 100000\n",
            "\n",
            "2021-03-19 02:13:48 - Score-Function: cos_sim\n",
            "2021-03-19 02:13:48 - Accuracy@1: 84.74%\n",
            "2021-03-19 02:13:48 - Accuracy@3: 92.80%\n",
            "2021-03-19 02:13:48 - Accuracy@5: 94.60%\n",
            "2021-03-19 02:13:48 - Accuracy@10: 96.40%\n",
            "2021-03-19 02:13:48 - Precision@1: 84.74%\n",
            "2021-03-19 02:13:48 - Precision@3: 38.59%\n",
            "2021-03-19 02:13:48 - Precision@5: 25.18%\n",
            "2021-03-19 02:13:48 - Precision@10: 13.60%\n",
            "2021-03-19 02:13:48 - Recall@1: 72.25%\n",
            "2021-03-19 02:13:48 - Recall@3: 86.96%\n",
            "2021-03-19 02:13:48 - Recall@5: 90.45%\n",
            "2021-03-19 02:13:48 - Recall@10: 93.69%\n",
            "2021-03-19 02:13:48 - MRR@10: 0.8908\n",
            "2021-03-19 02:13:48 - NDCG@10: 0.8876\n",
            "2021-03-19 02:13:48 - MAP@100: 0.8621\n",
            "2021-03-19 02:13:48 - Score-Function: dot_score\n",
            "2021-03-19 02:13:48 - Accuracy@1: 81.34%\n",
            "2021-03-19 02:13:48 - Accuracy@3: 90.78%\n",
            "2021-03-19 02:13:48 - Accuracy@5: 93.14%\n",
            "2021-03-19 02:13:48 - Accuracy@10: 95.24%\n",
            "2021-03-19 02:13:48 - Precision@1: 81.34%\n",
            "2021-03-19 02:13:48 - Precision@3: 37.39%\n",
            "2021-03-19 02:13:48 - Precision@5: 24.53%\n",
            "2021-03-19 02:13:48 - Precision@10: 13.34%\n",
            "2021-03-19 02:13:48 - Recall@1: 69.33%\n",
            "2021-03-19 02:13:48 - Recall@3: 84.66%\n",
            "2021-03-19 02:13:48 - Recall@5: 88.64%\n",
            "2021-03-19 02:13:48 - Recall@10: 92.33%\n",
            "2021-03-19 02:13:48 - MRR@10: 0.8649\n",
            "2021-03-19 02:13:48 - NDCG@10: 0.8638\n",
            "2021-03-19 02:13:48 - MAP@100: 0.8347\n",
            "2021-03-19 02:13:48 - Evaluate model with training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0a68e8c71e3454aa673bfd6a3fcacaf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='iâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "079a9bb30f364f04a80f74fd2f190732",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2.0, style=ProgressStyle(description_widtâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2021-03-19 02:13:49 - Binary Accuracy Evaluation of the model on  dataset after epoch 0:\n",
            "2021-03-19 02:14:41 - Accuracy with Cosine-Similarity:           76.61\t(Threshold: 0.8201)\n",
            "2021-03-19 02:14:41 - F1 with Cosine-Similarity:                 69.29\t(Threshold: 0.7391)\n",
            "2021-03-19 02:14:41 - Precision with Cosine-Similarity:          60.17\n",
            "2021-03-19 02:14:41 - Recall with Cosine-Similarity:             81.65\n",
            "2021-03-19 02:14:41 - Average Precision with Cosine-Similarity:  71.80\n",
            "\n",
            "2021-03-19 02:14:41 - Accuracy with Manhatten-Distance:           76.70\t(Threshold: 193.7392)\n",
            "2021-03-19 02:14:41 - F1 with Manhatten-Distance:                 69.44\t(Threshold: 240.4877)\n",
            "2021-03-19 02:14:41 - Precision with Manhatten-Distance:          59.75\n",
            "2021-03-19 02:14:41 - Recall with Manhatten-Distance:             82.88\n",
            "2021-03-19 02:14:41 - Average Precision with Manhatten-Distance:  71.87\n",
            "\n",
            "2021-03-19 02:14:42 - Accuracy with Euclidean-Distance:           76.75\t(Threshold: 8.8833)\n",
            "2021-03-19 02:14:42 - F1 with Euclidean-Distance:                 69.45\t(Threshold: 10.8946)\n",
            "2021-03-19 02:14:42 - Precision with Euclidean-Distance:          59.90\n",
            "2021-03-19 02:14:42 - Recall with Euclidean-Distance:             82.62\n",
            "2021-03-19 02:14:42 - Average Precision with Euclidean-Distance:  71.90\n",
            "\n",
            "2021-03-19 02:14:42 - Paraphrase Mining Evaluation on dev dataset after epoch 0:\n",
            "2021-03-19 02:14:57 - Number of candidate pairs: 294935\n",
            "2021-03-19 02:14:57 - Average Precision: 40.91\n",
            "2021-03-19 02:14:57 - Optimal threshold: 0.8637\n",
            "2021-03-19 02:14:57 - Precision: 46.41\n",
            "2021-03-19 02:14:57 - Recall: 45.03\n",
            "2021-03-19 02:14:57 - F1: 45.71\n",
            "\n",
            "2021-03-19 02:14:57 - Information Retrieval Evaluation on  dataset after epoch 0:\n",
            "2021-03-19 02:16:12 - Queries: 5000\n",
            "2021-03-19 02:16:12 - Corpus: 100000\n",
            "\n",
            "2021-03-19 02:16:13 - Score-Function: cos_sim\n",
            "2021-03-19 02:16:13 - Accuracy@1: 84.74%\n",
            "2021-03-19 02:16:13 - Accuracy@3: 92.80%\n",
            "2021-03-19 02:16:13 - Accuracy@5: 94.60%\n",
            "2021-03-19 02:16:13 - Accuracy@10: 96.40%\n",
            "2021-03-19 02:16:13 - Precision@1: 84.74%\n",
            "2021-03-19 02:16:13 - Precision@3: 38.59%\n",
            "2021-03-19 02:16:13 - Precision@5: 25.18%\n",
            "2021-03-19 02:16:13 - Precision@10: 13.60%\n",
            "2021-03-19 02:16:13 - Recall@1: 72.25%\n",
            "2021-03-19 02:16:13 - Recall@3: 86.96%\n",
            "2021-03-19 02:16:13 - Recall@5: 90.45%\n",
            "2021-03-19 02:16:13 - Recall@10: 93.69%\n",
            "2021-03-19 02:16:13 - MRR@10: 0.8908\n",
            "2021-03-19 02:16:13 - NDCG@10: 0.8876\n",
            "2021-03-19 02:16:13 - MAP@100: 0.8621\n",
            "2021-03-19 02:16:13 - Score-Function: dot_score\n",
            "2021-03-19 02:16:13 - Accuracy@1: 81.34%\n",
            "2021-03-19 02:16:13 - Accuracy@3: 90.78%\n",
            "2021-03-19 02:16:13 - Accuracy@5: 93.14%\n",
            "2021-03-19 02:16:13 - Accuracy@10: 95.24%\n",
            "2021-03-19 02:16:13 - Precision@1: 81.34%\n",
            "2021-03-19 02:16:13 - Precision@3: 37.39%\n",
            "2021-03-19 02:16:13 - Precision@5: 24.53%\n",
            "2021-03-19 02:16:13 - Precision@10: 13.34%\n",
            "2021-03-19 02:16:13 - Recall@1: 69.33%\n",
            "2021-03-19 02:16:13 - Recall@3: 84.66%\n",
            "2021-03-19 02:16:13 - Recall@5: 88.64%\n",
            "2021-03-19 02:16:13 - Recall@10: 92.33%\n",
            "2021-03-19 02:16:13 - MRR@10: 0.8649\n",
            "2021-03-19 02:16:13 - NDCG@10: 0.8638\n",
            "2021-03-19 02:16:13 - MAP@100: 0.8347\n",
            "2021-03-19 02:16:13 - Save model to output/training_OnlineConstrativeLoss-2021-03-19_02-11-20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1yg2rWwOwhA"
      },
      "source": [
        "## Check Evaluation Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "Dmbaak_pOvcn",
        "outputId": "eb3e8f45-05a0-4581-ad9b-4416e6f74437"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(model_save_path+'/binary_classification_evaluation_results.csv')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>steps</th>\n",
              "      <th>cosine_acc</th>\n",
              "      <th>cosine_acc_threshold</th>\n",
              "      <th>cosine_f1</th>\n",
              "      <th>cosine_precision</th>\n",
              "      <th>cosine_recall</th>\n",
              "      <th>cosine_f1_threshold</th>\n",
              "      <th>cosine_average_precision</th>\n",
              "      <th>manhatten_acc</th>\n",
              "      <th>manhatten_acc_threshold</th>\n",
              "      <th>manhatten_f1</th>\n",
              "      <th>manhatten_precision</th>\n",
              "      <th>manhatten_recall</th>\n",
              "      <th>manhatten_f1_threshold</th>\n",
              "      <th>manhatten_average_precision</th>\n",
              "      <th>eucledian_acc</th>\n",
              "      <th>eucledian_acc_threshold</th>\n",
              "      <th>eucledian_f1</th>\n",
              "      <th>eucledian_precision</th>\n",
              "      <th>eucledian_recall</th>\n",
              "      <th>eucledian_f1_threshold</th>\n",
              "      <th>eucledian_average_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.766114</td>\n",
              "      <td>0.834928</td>\n",
              "      <td>0.692862</td>\n",
              "      <td>0.601786</td>\n",
              "      <td>0.816421</td>\n",
              "      <td>0.739132</td>\n",
              "      <td>0.718011</td>\n",
              "      <td>0.766980</td>\n",
              "      <td>193.735626</td>\n",
              "      <td>0.694404</td>\n",
              "      <td>0.59753</td>\n",
              "      <td>0.828768</td>\n",
              "      <td>240.490311</td>\n",
              "      <td>0.718664</td>\n",
              "      <td>0.767493</td>\n",
              "      <td>8.883363</td>\n",
              "      <td>0.694471</td>\n",
              "      <td>0.598959</td>\n",
              "      <td>0.826221</td>\n",
              "      <td>10.894741</td>\n",
              "      <td>0.718970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.766141</td>\n",
              "      <td>0.820111</td>\n",
              "      <td>0.692859</td>\n",
              "      <td>0.601740</td>\n",
              "      <td>0.816498</td>\n",
              "      <td>0.739091</td>\n",
              "      <td>0.718011</td>\n",
              "      <td>0.767007</td>\n",
              "      <td>193.739166</td>\n",
              "      <td>0.694404</td>\n",
              "      <td>0.59753</td>\n",
              "      <td>0.828768</td>\n",
              "      <td>240.487747</td>\n",
              "      <td>0.718679</td>\n",
              "      <td>0.767493</td>\n",
              "      <td>8.883276</td>\n",
              "      <td>0.694471</td>\n",
              "      <td>0.598959</td>\n",
              "      <td>0.826221</td>\n",
              "      <td>10.894579</td>\n",
              "      <td>0.718988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch  steps  ...  eucledian_f1_threshold  eucledian_average_precision\n",
              "0      0      0  ...               10.894741                     0.718970\n",
              "1      0     -1  ...               10.894579                     0.718988\n",
              "\n",
              "[2 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QvypJrDlpU"
      },
      "source": [
        "## Load and Use Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ut9u5SKEOI"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses, util\n",
        "from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n",
        "from sentence_transformers.readers import InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import random"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R6N82BY5BM8J",
        "outputId": "56a25da5-3a46-4e6e-9e45-6202a474e065"
      },
      "source": [
        "# path of saved model\n",
        "model_save_path"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output/training_OnlineConstrativeLoss-2021-03-19_02-11-20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EN6y2jOBjzU",
        "outputId": "4ed4117e-a135-408b-a6e9-975cf65e7411"
      },
      "source": [
        "model = SentenceTransformer(model_save_path)\n",
        "# model = SentenceTransformer('http://www.server.com/path/to/model/my_model.zip')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-19 02:16:14 - Load pretrained SentenceTransformer: output/training_OnlineConstrativeLoss-2021-03-19_02-11-20\n",
            "2021-03-19 02:16:14 - Load SentenceTransformer from folder: output/training_OnlineConstrativeLoss-2021-03-19_02-11-20\n",
            "2021-03-19 02:16:15 - Use pytorch device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgz4HDRSFbrP",
        "outputId": "e33e5496-05e3-4ecc-c31b-7a026a822067"
      },
      "source": [
        "\"\"\"\n",
        "This application demonstrates how to find duplicate questions (paraphrases) in a long\n",
        "list of sentences.\n",
        "\"\"\"\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Questions can be a long list of sentences up to 100k sentences or more.\n",
        "# For demonstration purposes, we limit it to a few questions which all have on duplicate\n",
        "questions = [\n",
        "    'How did you catch your spouse cheating?',\n",
        "    'How can I find out if my husband is cheating?',\n",
        "    'Is my wife cheating?',\n",
        "    'How do I know if my partner is cheating?',\n",
        "    'Why is Starbucks in India overrated?',\n",
        "    'Is Starbucks overrated in india?',\n",
        "    'How can I lose weight fast without exercise?',\n",
        "    'Can I lose weight without exercise?',\n",
        "    'Which city is the best in India? Why?',\n",
        "    'Which is the best city in India?',\n",
        "    'How can I stay focused in class?',\n",
        "    'How can I stay focused on my school work?',\n",
        "    'How can I Remotely hack a mobile phone?',\n",
        "    'How can I hack my phone?',\n",
        "    'Where should I stay in Goa?',\n",
        "    'Which are the best hotels in Goa?',\n",
        "    'Why does hair turn white?',\n",
        "    'What causes older peoples hair to turn grey?',\n",
        "    'What is the easiest way to get followers on Quora?',\n",
        "    'How do I get more followers for my Quora?'\n",
        "]\n",
        "\n",
        "#model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')\n",
        "model = SentenceTransformer(model_save_path)\n",
        "\n",
        "# Given a model and a List of strings (texts), evaluation.ParaphraseMiningEvaluator.paraphrase_mining performs a\n",
        "# mining task by computing cosine similarity between all possible combinations and returning the ones with the highest scores.\n",
        "# It returns a list of tuples (score, i, j) with i, j representing the index in the questions list.\n",
        "pairs = util.paraphrase_mining(model, questions)\n",
        "\n",
        "#Output Top-20 pairs:\n",
        "for score, qid1, qid2 in pairs[0:20]:\n",
        "    print(\"{:.3f}\\t{}\\t\\t\\t{}\".format(score, questions[qid1], questions[qid2]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-19 02:16:15 - Load pretrained SentenceTransformer: output/training_OnlineConstrativeLoss-2021-03-19_02-11-20\n",
            "2021-03-19 02:16:15 - Load SentenceTransformer from folder: output/training_OnlineConstrativeLoss-2021-03-19_02-11-20\n",
            "2021-03-19 02:16:16 - Use pytorch device: cuda\n",
            "0.912\tWhich city is the best in India? Why?\t\t\tWhich is the best city in India?\n",
            "0.911\tHow can I lose weight fast without exercise?\t\t\tCan I lose weight without exercise?\n",
            "0.904\tWhy is Starbucks in India overrated?\t\t\tIs Starbucks overrated in india?\n",
            "0.884\tHow did you catch your spouse cheating?\t\t\tHow can I find out if my husband is cheating?\n",
            "0.839\tHow did you catch your spouse cheating?\t\t\tHow do I know if my partner is cheating?\n",
            "0.799\tHow can I Remotely hack a mobile phone?\t\t\tHow can I hack my phone?\n",
            "0.790\tHow can I find out if my husband is cheating?\t\t\tHow do I know if my partner is cheating?\n",
            "0.786\tWhat is the easiest way to get followers on Quora?\t\t\tHow do I get more followers for my Quora?\n",
            "0.778\tIs my wife cheating?\t\t\tHow do I know if my partner is cheating?\n",
            "0.772\tHow did you catch your spouse cheating?\t\t\tIs my wife cheating?\n",
            "0.760\tHow can I stay focused in class?\t\t\tHow can I stay focused on my school work?\n",
            "0.752\tWhere should I stay in Goa?\t\t\tWhich are the best hotels in Goa?\n",
            "0.746\tHow can I find out if my husband is cheating?\t\t\tIs my wife cheating?\n",
            "0.600\tWhich city is the best in India? Why?\t\t\tWhich are the best hotels in Goa?\n",
            "0.594\tWhich is the best city in India?\t\t\tWhich are the best hotels in Goa?\n",
            "0.473\tWhich city is the best in India? Why?\t\t\tWhere should I stay in Goa?\n",
            "0.438\tWhy is Starbucks in India overrated?\t\t\tWhich city is the best in India? Why?\n",
            "0.423\tWhich is the best city in India?\t\t\tWhere should I stay in Goa?\n",
            "0.385\tIs Starbucks overrated in india?\t\t\tWhich city is the best in India? Why?\n",
            "0.348\tHow did you catch your spouse cheating?\t\t\tHow can I hack my phone?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbBwYsUVDHE6"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQrdgSLNDHBj"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3vRWynoDG-a"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq1feCDLDG6r"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}