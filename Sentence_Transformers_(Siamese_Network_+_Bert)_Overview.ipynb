{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Transformers (Siamese Network + Bert) Overview.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTyFhk7PNjpXBzvGXcTAbV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dfe0fbe2521048609271ef8cc2778763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_876de3dcaaa9447a87c8cbcaa1ddea53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f988c11cac44644bfb0047c48fead47",
              "IPY_MODEL_42e60218648e49b2af4e238ce260d2c2"
            ]
          }
        },
        "876de3dcaaa9447a87c8cbcaa1ddea53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f988c11cac44644bfb0047c48fead47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cba02d7f1667426289e9df1adad6c48e",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95b4b58ad4cf4ad0b72776ab5c7396f4"
          }
        },
        "42e60218648e49b2af4e238ce260d2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02054f4099d04fb58a5af95ca88cb808",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00, 26.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0257c5802c724a0bb7fd276426915a31"
          }
        },
        "cba02d7f1667426289e9df1adad6c48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95b4b58ad4cf4ad0b72776ab5c7396f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02054f4099d04fb58a5af95ca88cb808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0257c5802c724a0bb7fd276426915a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/super-duper-transformer/blob/main/Sentence_Transformers_(Siamese_Network_%2B_Bert)_Overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfh6yAzs8FnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547d404c-180b-411a-d762-3d15c7e83491"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (0.4.1.2)\n",
            "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC_7xOCtTcMK"
      },
      "source": [
        "# Generate sentence embedding\n",
        "\n",
        "得到的是sentence embedding, 但是是通过Siamese network与Bert结合训练的embedding，由于loss function采用了 cosine-similarity loss function，生成的embedding是可以直接通过cosine similarity来比较**semantic similarity**的."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwBTm3jo8Fk7",
        "outputId": "36b0c252-9560-4cdb-e7b4-74692eecffac"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')  # paraphrase-distilroberta-base-v1 is a DistilBERT-base-uncased model fine tuned on a large dataset of paraphrase sentences."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 306M/306M [00:17<00:00, 17.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dfe0fbe2521048609271ef8cc2778763",
            "876de3dcaaa9447a87c8cbcaa1ddea53",
            "8f988c11cac44644bfb0047c48fead47",
            "42e60218648e49b2af4e238ce260d2c2",
            "cba02d7f1667426289e9df1adad6c48e",
            "95b4b58ad4cf4ad0b72776ab5c7396f4",
            "02054f4099d04fb58a5af95ca88cb808",
            "0257c5802c724a0bb7fd276426915a31"
          ]
        },
        "id": "t5Ax-MhE8Fhi",
        "outputId": "b77e2e4e-9323-4cf0-c501-14cacdba09a5"
      },
      "source": [
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.', \n",
        "    'The quick brown fox jumps over the lazy dog.',\n",
        "    '230u9nnJNKJNJK!@#@$']\n",
        "    \n",
        "sentence_embeddings = model.encode(sentences, show_progress_bar=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfe0fbe2521048609271ef8cc2778763",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1.0, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg6pUo2vTPtu"
      },
      "source": [
        "BERT (and other transformer networks) output for each token in our input text an embedding. In order to create a fixed-sized sentence embedding out of this, the model applies mean pooling, i.e., the output embeddings for all tokens are averaged to yield a 768-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IEXkJYi8Fey"
      },
      "source": [
        "# for sentence, embedding in zip(sentences, sentence_embeddings):\n",
        "#     print(\"Sentence:\", sentence)\n",
        "#     print(\"Embedding:\", embedding)\n",
        "#     print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOIzp3d78FcY",
        "outputId": "1062f631-449d-433c-d2fe-d654c1de5980"
      },
      "source": [
        "sentence_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jcrn29dWBg0"
      },
      "source": [
        "# Choose the Right Model\n",
        "\n",
        "## Paraphrase Identification\n",
        "\n",
        "The following models are recommended for various applications, as they were trained on Millions of paraphrase examples. They create extremely good results for various similarity and retrieval tasks. They are currently under development, better versions and more details will be released in future. But they many tasks they work better than the NLI / STSb models.\n",
        "\n",
        "- `paraphrase-distilroberta-base-v1` - Trained on large scale paraphrase data.\n",
        "- `paraphrase-xlm-r-multilingual-v1` - Multilingual version of distilroberta-base-paraphrase-v1, trained on parallel data for 50+ languages.\n",
        "\n",
        "## Semantic Textual Similarity\n",
        "The following models were optimized for Semantic Textual Similarity (STS). They were trained on SNLI+MultiNLI and then fine-tuned on the STS benchmark train set.\n",
        "\n",
        "The best available models for STS are:\n",
        "\n",
        "- `stsb-roberta-large` - STSb performance: 86.39\n",
        "- `stsb-roberta-base` - STSb performance: 85.44\n",
        "- `stsb-bert-large` - STSb performance: 85.29\n",
        "- `stsb-distilbert-base` - STSb performance: 85.16\n",
        "\n",
        "## Duplicate Questions Detection\n",
        "The following models were trained for duplicate questions mining and duplicate questions retrieval. You can use them to detect duplicate questions in a large corpus (see paraphrase mining) or to search for similar questions (see semantic search).\n",
        "\n",
        "Available models:\n",
        "\n",
        "- `quora-distilbert-base` - Model first tuned on NLI+STSb data, then fine-tune for Quora Duplicate Questions detection retrieval.\n",
        "- `quora-distilbert-multilingual` - Multilingual version of distilbert-base-nli-stsb-quora-ranking. Fine-tuned with parallel data for 50+ languages.\n",
        "\n",
        "## Question-Answer Retrieval - MSMARCO\n",
        "The following models were trained on MSMARCO Passage Ranking, a dataset with 500k real queries from Bing search. Given a search query, find the relevant passages.\n",
        "\n",
        "- `msmarco-distilroberta-base-v2`: MRR@10: 28.55 on MS MARCO dev set\n",
        "- `msmarco-roberta-base-v2`: MRR@10: 29.17 on MS MARCO dev set\n",
        "- `msmarco-distilbert-base-v2`: MRR@10: 30.77 on MS MARCO dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhbLmdqvW1pV",
        "outputId": "943d3cfc-b8f1-41d8-abc5-e5aa9ca32afd"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('msmarco-distilbert-base-v2')\n",
        "\n",
        "query_embedding = model.encode('How big is London')\n",
        "passage_embedding = model.encode('London has 9,787,426 inhabitants at the 2011 census')\n",
        "\n",
        "print(\"Similarity:\", util.pytorch_cos_sim(query_embedding, passage_embedding))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 245M/245M [00:16<00:00, 14.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Similarity: tensor([[0.6136]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ceUWq5WW4tq"
      },
      "source": [
        "## Question-Answer Retrieval - Natural Questions\n",
        "\n",
        "The following models were trained on Google’s Natural Questions dataset, a dataset with 100k real queries from Google search together with the relevant passages from Wikipedia.\n",
        "\n",
        "- `nq-distilbert-base-v1`: MRR10: 72.36 on NQ dev set (small)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46Q8S33sW8e_",
        "outputId": "10db3997-1608-41e8-bc02-bc4c64dc2dbc"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('nq-distilbert-base-v1')\n",
        "\n",
        "query_embedding = model.encode('How many people live in London?')\n",
        "\n",
        "#The passages are encoded as [ [title1, text1], [title2, text2], ...]\n",
        "passage_embedding = model.encode([['London', 'London has 9,787,426 inhabitants at the 2011 census.']])\n",
        "\n",
        "print(\"Similarity:\", util.pytorch_cos_sim(query_embedding, passage_embedding))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 245M/245M [00:16<00:00, 14.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Similarity: tensor([[0.6503]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Fl-D-iTVOb"
      },
      "source": [
        "# Comparing Sentence Similarities\n",
        "\n",
        "The sentences (texts) are mapped such that sentences with similar meanings are close in vector space. One common method to measure the similarity in vector space is to use cosine similarity. For two sentences, this can be done like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvg84eXr8FZw",
        "outputId": "10aec0c1-17b0-409f-bf83-f2314f41192b"
      },
      "source": [
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome',\n",
        "              'he is so smart',\n",
        "              'i work at a bank']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so amazing',\n",
        "              'he is a wise people',\n",
        "              'i like sitting and drinking at bank of river']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarits\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.4579\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.1759\n",
            "The new movie is awesome \t\t The new movie is so amazing \t\t Score: 0.9159\n",
            "he is so smart \t\t he is a wise people \t\t Score: 0.6925\n",
            "i work at a bank \t\t i like sitting and drinking at bank of river \t\t Score: 0.2215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWpNmYulVlc2"
      },
      "source": [
        "Brute-force的方法得到每个sentence的similar sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb9eA2y38FUL",
        "outputId": "57a09eac-36ac-4662-f4bf-ea31f2bc6260"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "# Single list of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9283\n",
            "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6855\n",
            "I love pasta \t\t Do you like pizza? \t\t Score: 0.5420\n",
            "I love pasta \t\t The new movie is awesome \t\t Score: 0.2629\n",
            "I love pasta \t\t The new movie is so great \t\t Score: 0.2268\n",
            "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1885\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.1759\n",
            "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1615\n",
            "The cat plays in the garden \t\t A woman watches TV \t\t Score: 0.1521\n",
            "The cat sits outside \t\t The new movie is awesome \t\t Score: 0.1475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1YCX85x8FQ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hKQ1MWXnrA"
      },
      "source": [
        "# Paraphrase Mining\n",
        "\n",
        "Use case: 给一堆sentences，快速的为每个sentence找到similar/duplicate sentence.\n",
        "\n",
        "Paraphrase mining is the task of finding pharaphrases (texts with identical / similar meaning) in a large corpus of sentences. In Semantic Textual Similarity we saw a simplified version of finding paraphrases in a list of sentences. The approach presented there used a brute-force approach to score and rank all pairs.\n",
        "\n",
        "However, as this has a quadratic runtime, it fails to scale to large (10,000 and more) collections of sentences.\n",
        "\n",
        "For larger collections, util offers the paraphrase_mining function that can be used like this:\n",
        "\n",
        "**Parameters**:\n",
        "- `model` – SentenceTransformer model for embedding computation\n",
        "- `sentences` – A list of strings (texts or sentences)\n",
        "- `show_progress_bar` – Plotting of a progress bar\n",
        "- `batch_size` – Number of texts that are encoded simultaneously by the model\n",
        "- `query_chunk_size` – Search for most similar pairs for #query_chunk_size at the same time. Decrease, to lower memory footprint (increases run-time).\n",
        "- `corpus_chunk_size` – Compare a sentence simultaneously against #corpus_chunk_size other sentences. Decrease, to lower memory footprint (increases run-time).\n",
        "- `max_pairs` – Maximal number of text pairs returned.\n",
        "- `top_k` – For each sentence, we retrieve up to top_k other sentences\n",
        "\n",
        "**Returns**: Returns a list of triplets with the format [score, id1, id2]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVo8YfTn8FN0",
        "outputId": "508e08d3-9dc7-4286-dab7-60cc564783ea"
      },
      "source": [
        "# Single list of sentences - Possible tens of thousands of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'I love chinese food',\n",
        "             'I love italian food',\n",
        "             'I love kung pao chicken',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "paraphrases = util.paraphrase_mining(model, sentences, top_k=1)\n",
        "\n",
        "for paraphrase in paraphrases[0:10]:\n",
        "    score, i, j = paraphrase\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.7063\n",
            "I love chinese food \t\t I love kung pao chicken \t\t Score: 0.6930\n",
            "I love pasta \t\t I love italian food \t\t Score: 0.6776\n",
            "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.5356\n",
            "Do you like pizza? \t\t I love italian food \t\t Score: 0.4335\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.3726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yysEYMbEZBTK"
      },
      "source": [
        "# Semantic Search\n",
        "\n",
        "Use case: 给一个new query/question，找出relevant document，听起来和paragraph mining挺相似的\n",
        "\n",
        "Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines, that only finds documents based on lexical matches, semantic search can also find synonyms.\n",
        "\n",
        "Background\n",
        "The idea behind semantic search is to embedd all entries in your corpus, which can be sentences, paragraphs, or documents, into a vector space.\n",
        "\n",
        "At search time, the query is embedded into the same vector space and the closest embedding from your corpus are found. These entries should have a high semantic overlap with the query.\n",
        "\n",
        "![](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png)\n",
        "## Symmetric vs. Asymmetric Semantic Search\n",
        "A cirtical distinction for your setup is symmetric vs. asymmetric semantic search:\n",
        "\n",
        "- For **symmetric semantic search** your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be “How to learn Python online?” and you want to find an entry like “How to learn Python on the web?”. For symmetric tasks, you could potentially flip the query and the entries in your corpus.\n",
        "\n",
        "- For **asymmetric semantic search**, you usually have a **short query** (like a question or some keywords) and you want to find a **longer paragraph** answering the query. An example would be a query like “What is Python” and you wand to find the paragraph “Python is an interpreted, high-level and general-purpose programming language. Python’s design philosophy …”. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.\n",
        "\n",
        "It is cirtical that you choose the right model for your type of task.\n",
        "Suitable models for symmetric semantic search:\n",
        "\n",
        "- `paraphrase-distilroberta-base-v1` / `paraphrase-xlm-r-multilingual-v1`\n",
        "- `quora-distilbert-base` / `quora-distilbert-multilingual`\n",
        "- `distiluse-base-multilingual-cased-v2`\n",
        "\n",
        "Suitable modesl for asymmetric semantic search:\n",
        "- `msmarco-distilbert-base-v2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbge_R218FKh",
        "outputId": "68e1f565-f859-4505-f59f-bd8e681bcf01"
      },
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: semantic search\n",
        "\n",
        "We have a corpus with various sentences. Then, for a given query sentence,\n",
        "we want to find the most similar sentence in this corpus.\n",
        "\n",
        "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "embedder = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = ['A man is eating food.',\n",
        "          'A man is eating italian food.',\n",
        "          'A man is eating a piece of bread.',\n",
        "          'The girl is carrying a baby.',\n",
        "          'A man is riding a horse.',\n",
        "          'A woman is playing violin.',\n",
        "          'Two men pushed carts through the woods.',\n",
        "          'A man is riding a white horse on an enclosed ground.',\n",
        "          'A monkey is playing drums.',\n",
        "          'A cheetah is running behind its prey.'\n",
        "          ]\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
        "\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(5, len(corpus))\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
        "\n",
        "    \"\"\"\n",
        "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "    hits = hits[0]      #Get the hits for the first query\n",
        "    for hit in hits:\n",
        "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: A man is eating pasta.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A man is eating food. (Score: 0.7096)\n",
            "A man is eating italian food. (Score: 0.6677)\n",
            "A man is eating a piece of bread. (Score: 0.6074)\n",
            "A man is riding a horse. (Score: 0.3360)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.3069)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Someone in a gorilla costume is playing a set of drums.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A monkey is playing drums. (Score: 0.6842)\n",
            "A woman is playing violin. (Score: 0.3762)\n",
            "A man is riding a horse. (Score: 0.3079)\n",
            "A cheetah is running behind its prey. (Score: 0.2760)\n",
            "A man is eating italian food. (Score: 0.2749)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: A cheetah chases prey on across a field.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A cheetah is running behind its prey. (Score: 0.7814)\n",
            "A monkey is playing drums. (Score: 0.2824)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.2208)\n",
            "A man is riding a horse. (Score: 0.2017)\n",
            "A man is eating food. (Score: 0.1886)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Llvznpk8FH3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN8y-Nxq8FDw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}