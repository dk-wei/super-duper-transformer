{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_classification_(CLS_+_Fine_tune_中文版本).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-7.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
    },
    "kernelspec": {
      "display_name": "dakwei-dev",
      "language": "python",
      "name": "dakwei-dev"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-wei/super-duper-transformer/blob/main/bert_classification_(CLS_%2B_Fine_tune_%E4%B8%AD%E6%96%87%E7%89%88%E6%9C%AC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Yc71sen9QL"
      },
      "source": [
        "Source Blog: [基于Transformers库的BERT模型：一个文本情感分类的实例解析](https://www.pythonf.cn/read/88960)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:31.026896Z",
          "start_time": "2021-04-02T22:39:31.024038Z"
        },
        "id": "_pKn0A-IuHWX",
        "scrolled": true
      },
      "source": [
        "#pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:32.187732Z",
          "start_time": "2021-04-02T22:39:31.041405Z"
        },
        "id": "dNlMtsjqt6Mm"
      },
      "source": [
        "#part2: bert feature-base\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as tfs\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0CjKsdfvnvt"
      },
      "source": [
        "# Part 1: 直接利用BERT提取`[CLS]`特征的方式进行建模"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQAC5ueLwRBz"
      },
      "source": [
        "## 加载数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:32.595353Z",
          "start_time": "2021-04-02T22:39:32.189581Z"
        },
        "id": "Bbew3qKyt7W9"
      },
      "source": [
        "train_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:32.601699Z",
          "start_time": "2021-04-02T22:39:32.597803Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6L9PZOGt7US",
        "outputId": "09331223-81d2-44db-e823-ea7f85aab5f0"
      },
      "source": [
        "train_set = train_df[:3000]\n",
        "\n",
        "print(\"Train set shape:\", train_set.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set shape: (3000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:32.612699Z",
          "start_time": "2021-04-02T22:39:32.603969Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlONsmFBt7R1",
        "outputId": "22cbada6-e41c-4ea1-84c0-729c861c62da"
      },
      "source": [
        "train_set[1].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1565\n",
              "0    1435\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJiRR_3AwYdj"
      },
      "source": [
        "可以看出，积极和消极的标签基本对半分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbi2l3mowczJ"
      },
      "source": [
        "## 利用BERT进行特征抽取\n",
        "\n",
        "在这里，我们利用BERT对数据集进行特征抽取，即把输入数据经过BERT模型，来获取输入数据的特征，这些特征包含了整个句子的信息，是语境层面的。这种做法类似于EMLo的特征抽取。需要注意的是，这里并没有使用到BERT的微调，因为BERT并不参与后面的训练，仅仅进行特征抽取操作。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:41.238408Z",
          "start_time": "2021-04-02T22:39:32.614898Z"
        },
        "id": "LWmwpesnt7PX"
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, tfs.BertTokenizer, 'bert-base-uncased')\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJVUbgPKwj2N"
      },
      "source": [
        "我们使用预训练好的\"`bert-base-uncased`\"模型参数进行处理，采用的模型是 `BertModel` ，采用的分词器是 `BertTokenizer` 。由于我们的输入句子是英文句子，所以需要先分词；然后把单词映射成词汇表的索引，再喂给模型。实际上Bert的分词操作，不是以传统的单词为单位的，而是以 `wordpiece` 为单位，这是比单词更细粒度的单位。我们执行以下代码："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:42.535095Z",
          "start_time": "2021-04-02T22:39:41.240215Z"
        },
        "id": "kig5xbpst7N_"
      },
      "source": [
        "train_tokenized = train_set[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B6ntVrqwv4f"
      },
      "source": [
        "然后，为了提升训练速度，我们需要把句子都处理成同一个长度，即常见的pad操作，我们在短的句子末尾添加一系列的`[PAD]`符号："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:42.561234Z",
          "start_time": "2021-04-02T22:39:42.536689Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EHkZSAQt7LM",
        "outputId": "de0c3503-b135-47cc-982a-8f670b05c193"
      },
      "source": [
        "train_max_len = 0\n",
        "for i in train_tokenized.values:\n",
        "    if len(i) > train_max_len:\n",
        "        train_max_len = len(i)\n",
        "\n",
        "train_padded = np.array([i + [0] * (train_max_len-len(i)) for i in train_tokenized.values])\n",
        "print(\"train set shape:\",train_padded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set shape: (3000, 66)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JGKn-m8w6yD"
      },
      "source": [
        "最后，我们还需要让模型知道，哪些词是不用处理的，即上面我们添加的[PAD]符号, 其实就是attention-mask的功能：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:39:42.584053Z",
          "start_time": "2021-04-02T22:39:42.563150Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcKDcs-Ct7Ha",
        "outputId": "6837591b-14f7-4064-ac73-b9c23560e79a"
      },
      "source": [
        "print(train_padded[0])\n",
        "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
        "print(train_attention_mask[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  101  1037 18385  1010  6057  1998  2633 18276  2128 16603  1997  5053\n",
            "  1998  1996  6841  1998  5687  5469  3152   102     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:46:56.951215Z",
          "start_time": "2021-04-02T22:39:42.586599Z"
        },
        "id": "pJoLt2EQt7E0"
      },
      "source": [
        "train_input_ids = torch.tensor(train_padded).long()\n",
        "train_attention_mask = torch.tensor(train_attention_mask).long()\n",
        "with torch.no_grad():\n",
        "    train_last_hidden_states = model(train_input_ids, attention_mask=train_attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwTPrhfxRtQ"
      },
      "source": [
        "我们来看以下Bert模型给我们的输出是什么样的："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:46:56.960070Z",
          "start_time": "2021-04-02T22:46:56.953983Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKIpiTX-t7Bj",
        "outputId": "3ebc6eb3-fc4e-48ca-e8a1-f1887b09428c"
      },
      "source": [
        "train_last_hidden_states[0].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3000, 66, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afFjZvrJxToL"
      },
      "source": [
        "第一维的是样本数量，第二维的是序列长度，第三维是特征数量。也就是说，Bert对于我们的每一个位置的输入，都会输出一个对应的特征向量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70PgTqNFxf9b"
      },
      "source": [
        "## 切分数据成训练集和测试集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:46:57.014370Z",
          "start_time": "2021-04-02T22:46:56.961800Z"
        },
        "id": "-hj_thNkt6-x"
      },
      "source": [
        "train_features = train_last_hidden_states[0][:,0,:].numpy()\n",
        "train_labels = train_set[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQIutGHdxicV"
      },
      "source": [
        "请注意：我们使用`[:,0,:]`来提取序列第一个位置的输出向量，因为第一个位置是[CLS]，比起其他位置，该向量应该更具有代表性，蕴含了整个句子的信息。紧接着，我们利用sklearn库的方法来把数据集切分成训练集和测试集。\n",
        "\n",
        "![](https://camo.githubusercontent.com/009eabd7b7697055256ab3655f048d5c31967e1b/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d6f75747075742d74656e736f722d73656c656374696f6e2e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:46:57.052278Z",
          "start_time": "2021-04-02T22:46:57.016080Z"
        },
        "id": "h9GUB74Ht6JI"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(train_features, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydmgBc8DySOr"
      },
      "source": [
        "## 使用逻辑回归进行训练\n",
        "\n",
        "在这一部分，我们使用sklearn的逻辑回归模块对我们的训练集进行拟合，最后在测试集上进行评价："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:46:57.383282Z",
          "start_time": "2021-04-02T22:46:57.053829Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lvQYTUIt6G2",
        "outputId": "a5d2c26c-d65d-4f37-874d-d2e716b084fd"
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:46:57.397401Z",
          "start_time": "2021-04-02T22:46:57.385431Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCO6m_kht6D5",
        "outputId": "694db145-4dac-4132-bd79-56d6a9e2ffc0"
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8213333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zied6rcIyV9l"
      },
      "source": [
        "经过逻辑回归模型的拟合，其准确率达到了79.21，分类效果还不错。那么，我们还能进一步提升吗？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI-YRYs5vuw4"
      },
      "source": [
        "# Part 2: 利用BERT基于微调的方式进行建模\n",
        "\n",
        "在上一部分，我们利用了Bert抽取特征的能力进行建模，直接提取了Bert的输出特征，再输入给一个线性层以预测。但Bert本身的不参与模型的训练。\n",
        "\n",
        "现在我们采取另一种方式，即fine-tuned，Bert与线性层一起参与训练，反向传播会更新bert和classifier二者的参数，使得Bert模型更加适合这个分类任务。那么，让我们开始吧~\n",
        "\n",
        "## 建立模型\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T23:04:01.541638Z",
          "start_time": "2021-04-02T23:04:01.536022Z"
        },
        "id": "t9r0spN_t6BU"
      },
      "source": [
        "#part 2 - bert fine-tuned\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import transformers as tfs\n",
        "import math\n",
        "\n",
        "class BertClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClassificationModel, self).__init__()   \n",
        "        model_class, tokenizer_class, pretrained_weights = (tfs.BertModel, \n",
        "                                                            tfs.BertTokenizer, \n",
        "                                                            'bert-base-uncased'\n",
        "                                                           )         \n",
        "        # tokenization\n",
        "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "        # load model\n",
        "        self.bert = model_class.from_pretrained(pretrained_weights)\n",
        "        self.dense = nn.Linear(768, 2)  #pytorch中为线性变换Linear layer，bert默认的隐藏单元数是768， 输出到2个单元，表示二分类\n",
        "        \n",
        "    def forward(self, batch_sentences):\n",
        "        # 整个BertClassificationModel都是forward-propagation，直到最后输出softmax输出\n",
        "        # 下面training会接入back-propagation\n",
        "        #print(batch_sentences[4])\n",
        "\n",
        "        #上面那些构造attention mask以及tokenization直接可以用batch_encode_plus一步到位\n",
        "        batch_tokenized = self.tokenizer.batch_encode_plus(batch_sentences, \n",
        "                                                           add_special_tokens=True,\n",
        "                                                           max_length=66, \n",
        "                                                           padding='max_length', # padding有很多种，可以自行查阅documents\n",
        "                                                           truncation=True,\n",
        "                                                           #pad_to_max_length=True\n",
        "                                                           )      #tokenize、add special token、pad\n",
        "        \n",
        "        input_ids = torch.tensor(batch_tokenized['input_ids'])\n",
        "        attention_mask = torch.tensor(batch_tokenized['attention_mask'])\n",
        "        \n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        bert_cls_hidden_state = bert_output[0][:,0,:]       #提取[CLS]对应的隐藏状态, 即使是training，用的也是[CLS]的embedding\n",
        "        \n",
        "        linear_output = self.dense(bert_cls_hidden_state)    #dense/Linear一下CLS的hidden state就可以直接输出类似softmax probability了\n",
        "        #print('linear_output:' linear_output)\n",
        "        return linear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXIdj1Ey0Bh-"
      },
      "source": [
        "模型很简单，关键代码都在上面注释了。其主要构成是在bert模型的[CLS]输出位置接上一个线性层，用以预测句子的分类。\n",
        "\n",
        "## 数据分批\n",
        "\n",
        "下面我们对原来的数据集进行一些改造，分成batch_size为64大小的数据集，以便模型进行批量梯度下降。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T23:04:03.000579Z",
          "start_time": "2021-04-02T23:04:02.994349Z"
        },
        "id": "SpwzRzdlt5-d"
      },
      "source": [
        "sentences = train_set[0].values\n",
        "targets = train_set[1].values\n",
        "train_inputs, test_inputs, train_targets, test_targets = train_test_split(sentences, targets)\n",
        "\n",
        "batch_size = 128\n",
        "batch_count = int(len(train_inputs) / batch_size)\n",
        "batch_train_inputs, batch_train_targets = [], []\n",
        "\n",
        "for i in range(batch_count):\n",
        "    batch_train_inputs.append(train_inputs[i*batch_size : (i+1)*batch_size])\n",
        "    batch_train_targets.append(train_targets[i*batch_size : (i+1)*batch_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T23:04:03.624218Z",
          "start_time": "2021-04-02T23:04:03.620737Z"
        },
        "id": "aQgnhJlmPTjU",
        "outputId": "d4908e36-3ca0-480c-9487-f7cdafaf59a5"
      },
      "source": [
        "batch_train_inputs[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVEH-MD0FQ0"
      },
      "source": [
        "## 训练模型\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T00:51:52.884297Z",
          "start_time": "2021-04-02T23:47:19.457650Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "25b4a0d309914d3c914b0d76ed4ecc87",
            "8f8b9aa7083d4dbbbd2bcee21bd1d8c6",
            "e26fc6b9fc7e4d5884a9c2c8588a01ee",
            "feffa17396cf43939844f379178de34d",
            "27a6379070d74d3e855b52648ab6b37a",
            "f63ff759cdb94c6998e601cafd05fd11",
            "73acff54a826477b9b6074834c9040b2",
            "297eee2ad18c462187976bad5de39a25",
            "f96646f043c94c36836077206ae4c73c"
          ]
        },
        "id": "K2OPOyOHt57g",
        "scrolled": false,
        "outputId": "c1177762-689a-4704-85e5-a319b0c82d8a"
      },
      "source": [
        "#train the model\n",
        "epochs = 8\n",
        "lr = 0.01\n",
        "print_every_batch = 10\n",
        "\n",
        "bert_classifier_model = BertClassificationModel()\n",
        "optimizer = optim.SGD(bert_classifier_model.parameters(), \n",
        "                      lr=lr\n",
        "                     )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print_avg_loss = 0\n",
        "    for i in tqdm(range(batch_count)):\n",
        "        # forward-propagation\n",
        "        inputs = batch_train_inputs[i]\n",
        "        labels = torch.tensor(batch_train_targets[i])\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = bert_classifier_model(inputs)\n",
        "        #print(outputs, labels)\n",
        "        loss = criterion(outputs, labels)  # outputs 是[[0.3, 0.7], [0.2, 0.8]]这样的prob tuple，而labels则是[1, 0]这样的label\n",
        "\n",
        "        # back-propagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        print_avg_loss += loss.item()\n",
        "        if i % print_every_batch == (print_every_batch-1):\n",
        "            print(\"Batch: %d, Loss: %.4f\" % ((i+1), print_avg_loss/print_every_batch))\n",
        "            print_avg_loss = 0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25b4a0d309914d3c914b0d76ed4ecc87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f8b9aa7083d4dbbbd2bcee21bd1d8c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.6940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e26fc6b9fc7e4d5884a9c2c8588a01ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.6086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feffa17396cf43939844f379178de34d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.4133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27a6379070d74d3e855b52648ab6b37a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.4082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f63ff759cdb94c6998e601cafd05fd11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.2439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73acff54a826477b9b6074834c9040b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.2007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "297eee2ad18c462187976bad5de39a25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.1639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f96646f043c94c36836077206ae4c73c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 10, Loss: 0.3244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsk90tbD0I2S"
      },
      "source": [
        "## 模型评价\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T00:53:31.174270Z",
          "start_time": "2021-04-03T00:52:14.807990Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w07k28gt54t",
        "outputId": "dad2b1fe-37bd-4915-9f15-2f49240ebad6"
      },
      "source": [
        "# eval the trained model\n",
        "total = len(test_inputs)\n",
        "hit = 0\n",
        "with torch.no_grad():\n",
        "    for i in range(total):\n",
        "        outputs = bert_classifier_model([test_inputs[i]])\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        if predicted == test_targets[i]:\n",
        "            hit += 1\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (hit / total * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 87.73%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXVUpfOPoLUs"
      },
      "source": [
        "可以看出，通过微调的方式来建模，经过3个轮次的训练后，模型的准确率达到了87.73%，比起基于特征的建模方式有了较大提升。下面给出本文代码的地址，有需要的可以自取~谢谢您的阅读！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-02T22:48:48.449147Z",
          "start_time": "2021-04-02T22:39:30.925Z"
        },
        "id": "os5vgYc6t51s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdl4YE_EPTjX"
      },
      "source": [
        "# 附 `nn.CrossEntropyLoss()`的input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T00:56:18.788977Z",
          "start_time": "2021-04-03T00:56:18.785303Z"
        },
        "id": "mKaeFLpfPTjX"
      },
      "source": [
        ">>> loss = nn.CrossEntropyLoss()\n",
        ">>> softmax_outputs = torch.randn(3, 5, requires_grad=True)\n",
        ">>> target = torch.empty(3, dtype=torch.long).random_(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T00:56:19.156676Z",
          "start_time": "2021-04-03T00:56:19.152704Z"
        },
        "id": "c0K7WN5OPTjX",
        "outputId": "65b4e45a-dd95-4682-d793-ac5e23f546f8"
      },
      "source": [
        "softmax_outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3512, -0.2773,  0.8065, -0.7479,  2.5387],\n",
              "        [-1.0910, -0.1425, -0.3537, -1.3583,  2.0077],\n",
              "        [ 0.5450, -0.6343, -0.3883, -0.4893,  0.9117]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T00:56:19.824358Z",
          "start_time": "2021-04-03T00:56:19.820563Z"
        },
        "id": "j39CQI56PTjX",
        "outputId": "68d3d705-69cc-4a2f-adbe-76d8f23df3a5"
      },
      "source": [
        "target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 4, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJfTyPIYPTjY"
      },
      "source": [
        "可以看到softmax_outputs和output的shape不是一致的，前者为各个class的概率，后者直接为target label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuObJHhtPTjY"
      },
      "source": [
        ""
      ]
    }
  ]
}